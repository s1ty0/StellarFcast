# åœ¨æœ€é¡¶éƒ¨å¿½ç•¥ torchvision å›¾åƒæ‰©å±•åŠ è½½å¤±è´¥çš„è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="torchvision.io.image")

import os
import torch
import random

# å¯ç”¨ Tensor Core åŠ é€Ÿï¼ˆæ¨è 'high'ï¼‰ï¼Œå¯¹æ€§èƒ½çš„å½±å“å¾®ä¹å…¶å¾®ï¼Œèƒ½å……åˆ†åˆ©ç”¨GPUèƒ½åŠ›
torch.set_float32_matmul_precision('high')

import torch.nn as nn
import argparse
import numpy as np
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from peft import get_peft_model, LoraConfig
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score, \
    roc_auc_score, fbeta_score
from pytorch_lightning import LightningModule, LightningDataModule, Trainer
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger

from transformers import BertModel, GPT2Model, RobertaModel

# å¼•å…¥æ”¹è¿›åçš„æ•°æ®åŠ è½½å‡½æ•°
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None

# å¼•å…¥ç‰©ç†æŸå¤±å‡½æ•°ï¼š
from phy_loss import PhysicsRegularizedLoss # æ­¤å¤„çš„phy_losså³æ˜¯ç”¨äº†ç¬¬äºŒç‰ˆæœ¬çš„v2_loss

# å®šä¹‰æ¨¡å‹-è·¯å¾„åŒ¹é…è¡¨
MODEL_PATH_MAP = {
    "bert": "./models/bert_base_uncased",
    "gpt2": "./models/gpt2",
    "roberta": "./models/roberta-base",
}

def collate_fn(data):
        """
        åŒæ—¶æ”¯æŒå•æ¨¡æ€ï¼ˆä»…æ—¶åºç‰¹å¾ï¼‰å’Œå¤šæ¨¡æ€ï¼ˆæ—¶åº+æ–‡æœ¬ç‰¹å¾ï¼‰è¾“å…¥
        - å•æ¨¡æ€ï¼šæ¯ä¸ªæ ·æœ¬ä¸º (features, label)ï¼Œå…¶ä¸­ features æ˜¯æ—¶åºæ•°æ®
        - å¤šæ¨¡æ€ï¼šæ¯ä¸ªæ ·æœ¬ä¸º ({"x_enc": æ—¶åºæ•°æ®, "text_emb": æ–‡æœ¬åµŒå…¥}, label)
        - è‹¥æ²¡æœ‰æ–‡æœ¬åµŒå…¥ï¼Œtext_emb_batch è¿”å› None
        """
        # è§£åŒ…æ•°æ®ï¼šåŒºåˆ†å•æ¨¡æ€å’Œå¤šæ¨¡æ€æ ¼å¼
        if isinstance(data[0][0], dict):
            # å¤šæ¨¡æ€æ ¼å¼ï¼š(dict, label)
            inputs, labels = zip(*data)
            # æå–æ—¶åºç‰¹å¾
            x_enc_list = [inp["x_enc"] for inp in inputs]
            # æå–æ–‡æœ¬åµŒå…¥ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
            text_emb_list = [inp.get("text_emb") for inp in inputs]
            his_emb_list = [inp.get("his_emb") for inp in inputs]

            # ä» dict ä¸­æå– raw_lc
            raw_lc_list = [inp["raw_lc"] for inp in inputs]
        else:
            # å•æ¨¡æ€æ ¼å¼ï¼š(features, label)ï¼Œé»˜è®¤featuresä¸ºæ—¶åºæ•°æ®
            features, labels = zip(*data)
            x_enc_list = features
            text_emb_list = [None] * len(features)  # å•æ¨¡æ€æ—¶æ–‡æœ¬åµŒå…¥å…¨ä¸ºNone
            his_emb_list = [None] * len(features)  # å•æ¨¡æ€æ—¶æ–‡æœ¬åµŒå…¥å…¨ä¸ºNone

            raw_lc_list = features  # fallback: raw_lc ç­‰äºè¾“å…¥ç‰¹å¾

        # å¤„ç†æ—¶åºè¾“å…¥ï¼šå †å ä¸º (B, L, C)
        x_enc_batch = torch.stack([
            torch.as_tensor(x, dtype=torch.float32) for x in x_enc_list
        ], dim=0)

        # å †å  raw_lc: åº”ä¸º (B, L) â€”â€” ç¡®ä¿åŸå§‹å…‰å˜æ›²çº¿æ˜¯äºŒç»´
        raw_lc_batch = torch.stack([
            torch.as_tensor(x, dtype=torch.float32) for x in raw_lc_list
        ], dim=0)

        # å¤„ç†æ–‡æœ¬åµŒå…¥(statistics)ï¼šå…¨ä¸ºNoneåˆ™è¿”å›Noneï¼Œå¦åˆ™å †å ä¸º (B, D)
        if all(emb is None for emb in text_emb_list):
            text_emb_batch = None
        else:
            # è¿‡æ»¤æ‰Noneï¼ˆç†è®ºä¸Šä¸ä¼šå‡ºç°éƒ¨åˆ†æœ‰éƒ¨åˆ†æ— çš„æƒ…å†µï¼‰
            text_emb_batch = torch.stack([
                torch.as_tensor(emb, dtype=torch.float32)
                for emb in text_emb_list if emb is not None
            ], dim=0)
            # è‹¥å­˜åœ¨Noneä½†ä¸å…¨ä¸ºNoneï¼ˆå¼‚å¸¸æƒ…å†µï¼‰ï¼Œè¡¥å……è­¦å‘Š
            if len(text_emb_batch) != len(text_emb_list):
                import warnings
                warnings.warn("éƒ¨åˆ†æ ·æœ¬æ–‡æœ¬åµŒå…¥ä¸ºNoneï¼Œå·²è‡ªåŠ¨è¿‡æ»¤")

        # å¤„ç†æ–‡æœ¬åµŒå…¥(history)ï¼šå…¨ä¸ºNoneåˆ™è¿”å›Noneï¼Œå¦åˆ™å †å ä¸º (B, D)
        if all(emb is None for emb in his_emb_list):
            his_emb_batch = None
        else:
            # è¿‡æ»¤æ‰Noneï¼ˆç†è®ºä¸Šä¸ä¼šå‡ºç°éƒ¨åˆ†æœ‰éƒ¨åˆ†æ— çš„æƒ…å†µï¼‰
            his_emb_batch = torch.stack([
                torch.as_tensor(emb, dtype=torch.float32)
                for emb in his_emb_list if emb is not None
            ], dim=0)
            # è‹¥å­˜åœ¨Noneä½†ä¸å…¨ä¸ºNoneï¼ˆå¼‚å¸¸æƒ…å†µï¼‰ï¼Œè¡¥å……è­¦å‘Š
            if len(his_emb_batch) != len(his_emb_list):
                import warnings
                warnings.warn("éƒ¨åˆ†æ ·æœ¬æ–‡æœ¬åµŒå…¥ä¸ºNoneï¼Œå·²è‡ªåŠ¨è¿‡æ»¤")

        # å¤„ç†æ ‡ç­¾ï¼šå †å ä¸º (B, num_label)
        y_batch = torch.stack(labels, dim=0)

        return {"x_enc": x_enc_batch, "text_emb": text_emb_batch, "his_emb": his_emb_batch, "raw_lc": raw_lc_batch}, y_batch

# å®šä¹‰æ•°æ®é›†
class FluxDataLoader(Dataset):
    def __init__(self, root_path, flag=None, on_enhance=False, encoder="minLM", on_mm_statistics=False, on_mm_history=False, on_test_data_half=False, on_downSample=False): #
        self.flag = flag
        self.encoder = encoder
        self.on_mm_statistics = on_mm_statistics # æ˜¯å¦å¼€å¯å¤šæ¨¡æ€
        self.on_mm_history = on_mm_history
        self.on_enhance = on_enhance # æ˜¯å¦å¼€å¯å•æ¨¡æ€æ•°æ®å¢å¼ºï¼ˆå¼•å…¥å·®åˆ†ï¼‰
        self.on_test_data_half = on_test_data_half
        self.on_downSample = on_downSample

        # === æ–‡æœ¬ç¼–ç å™¨è·¯å¾„æ˜ å°„ï¼ˆå¯æ‰©å±•ï¼‰===
        ENCODER_PATH_MAP = { #
            "minLM": "./textEncoder/all-MiniLM-L6-v2",
            "bert-chinese": "./textEncoder/bert-base-chinese",
            # æœªæ¥å¯åŠ ï¼š "bge": "./textEncoder/bge-small-en-v1.5", ...
        }

        if self.encoder not in ENCODER_PATH_MAP:
            raise ValueError(f"Unsupported encoder: {self.encoder}. Choose from {list(ENCODER_PATH_MAP.keys())}")

        self.text_encoder_path = ENCODER_PATH_MAP[self.encoder]

        # ç¡®å®šæ•°æ®è·¯å¾„
        if flag == 'TRAIN':
            if self.on_downSample:
                data_dir = f"{root_path}/train_sampled_data"
            else:
                data_dir = f"{root_path}/train"
        elif flag == 'TEST':
            if self.on_test_data_half:
                data_dir = f"{root_path}/test_half"
            else:
                data_dir = f"{root_path}/test"
        elif flag == 'VAL':
            data_dir = f"{root_path}/val"
        else:
            data_dir = root_path

        # åŠ è½½æ•°æ®
        lc_data = np.load(f"{data_dir}/lc_data.npy")      # (N, 512)
        label_data = np.load(f"{data_dir}/label_data.npy")  # (N,)

        # âœ… debug è‹¥å¼€å¯ï¼Œåˆ™æ‰§è¡Œå°æ ·æœ¬
        # lc_data = lc_data[0:10]
        # label_data = label_data[0:10]

        self.X = lc_data      # (N, 512)
        self.y = label_data   # (N,)
        print(f"[{flag}] Loaded {len(self.X)} samples.")

        # === é¢„è®¡ç®—æ–‡æœ¬åµŒå…¥ï¼ˆç”¨äº use_multimodalï¼‰===
        self.text_embeddings = None
        self.history_embeddings = None

        # ============== ä¿®æ”¹å  =================
        encoder="bert-chinese" # é»˜è®¤æ‰€æœ‰ç¼–ç å™¨ä½¿ç”¨çš„éƒ½æ˜¯bert, å…¬å¹³æ¯”è¾ƒ
        if self.on_mm_statistics:
            emb_file = os.path.join(data_dir, f"text_embeddings_{encoder}.npy")
            if os.path.exists(emb_file):
                self.text_embeddings = np.load(emb_file)
                print(f"[{flag}] Loaded text embeddings from {emb_file}, shape: {self.text_embeddings.shape}")
            else:
                raise FileNotFoundError(
                    f"Text embeddings not found at {emb_file}. Please run generate_text_embeddings.py first.")

        if self.on_mm_history: #  f"text_embeddings_his_red_{args.encoder}.npy"
            emb_file = os.path.join(data_dir, f"text_embeddings_his_red_{encoder}.npy")
            if os.path.exists(emb_file):
                self.history_embeddings = np.load(emb_file)
                print(f"[{flag}] Loaded (history) text embeddings from {emb_file}, shape: {self.history_embeddings.shape}")
            else:
                raise FileNotFoundError(
                    f"Text embeddings not found at {emb_file}. Please run generate_history_embeddings.py first.")

    def _enhance_flux(self, x):
        """
        ä»…ä¿ç•™åŸå§‹ä¿¡å·å’Œä¸€é˜¶å·®åˆ†ï¼ˆ2é€šé“ï¼‰
        x: (512,) æˆ– (1, 512) â†’ ç»Ÿä¸€å¤„ç†ä¸º (512,)
        Returns: (512, 2)
        """
        # æ ‡å‡†åŒ–è¾“å…¥ä¸º (512,)
        if x.ndim == 2:
            if x.shape[0] == 1:
                x = x.squeeze(0)  # (1, 512) -> (512,)
            elif x.shape[1] == 512:
                x = x[0]  # ä¿å®ˆå¤„ç†
            else:
                raise ValueError(f"Unexpected x shape: {x.shape}")
        elif x.ndim != 1:
            raise ValueError(f"Invalid x ndim: {x.ndim}")

        # 1. åŸå§‹ä¿¡å·
        feat1 = x  # (512,)

        # 2. ä¸€é˜¶å·®åˆ†
        feat2 = np.zeros_like(x)
        feat2[1:] = np.diff(x)

        # æ‹¼æ¥ä¸º (512, 2)
        features = np.stack([feat1, feat2], axis=1)

        return features

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx): #
        x_raw = self.X[idx]  # (1ï¼Œ512)
        y = self.y[idx]
        y = int(y)

        # ç¡®ä¿x_rawå˜æˆ (512,)
        if x_raw.ndim == 2 and x_raw.shape[0] == 1:
            x_raw = x_raw.squeeze(0)  # (1, 512) â†’ (512,)
        elif x_raw.ndim == 2 and x_raw.shape[1] == 1:
            x_raw = x_raw.squeeze(1)  # (512, 1) â†’ (512,)

        # ä¿ç•™åŸå§‹å…‰å˜æ›²çº¿, ç‰©ç†æŸå¤±å‡½æ•°éœ€è¦ç”¨åˆ°
        raw_lc = x_raw.copy()  # (512,) â€”â€” æ³¨æ„ï¼šç¡®ä¿æ˜¯ numpy array æˆ–å¯è½¬ tensor

        # æ‰©å……ä¸€ä¸ªç»´åº¦
        x_final = x_raw[:, None]  # (512, 1)

        # 3ä¸ªæ”¹è¿›ç‚¹éƒ½å¼€å¯ [mm, mm_history, enhance]
        if self.on_mm_statistics and self.on_enhance and self.on_mm_history:
            x_final = self._enhance_flux(x_raw)  # (512, 2)
            text_emb = self.text_embeddings[idx].astype(np.float32)  # (384,)
            his_emb = self.history_embeddings[idx].astype(np.float32)  # (384,)
            return {"x_enc": x_final, "text_emb": text_emb, "his_emb": his_emb, "raw_lc": raw_lc}, torch.tensor(y, dtype=torch.long)

        # å¼€å¯ä¸¤ä¸ª [mm, enhance]
        if self.on_mm_statistics and self.on_enhance:
            x_final = self._enhance_flux(x_raw)  # (512, 2)
            text_emb = self.text_embeddings[idx].astype(np.float32)  # (384,)
            return {"x_enc": x_final, "text_emb": text_emb,"his_emb": None, "raw_lc": raw_lc}, torch.tensor(y, dtype=torch.long)

        # å¼€å¯ä¸¤ä¸ª [mm_his, enhance]
        if self.on_mm_history and self.on_enhance:
            x_final = self._enhance_flux(x_raw)  # (512, 2)
            his_emb = self.history_embeddings[idx].astype(np.float32)  # (384,)
            return {"x_enc": x_final, "text_emb": None, "his_emb": his_emb, "raw_lc": raw_lc}, torch.tensor(y, dtype=torch.long)

        # å¼€å¯ä¸¤ä¸ª [mm, mm_his]
        if self.on_mm_history and self.on_mm_statistics:
            text_emb = self.text_embeddings[idx].astype(np.float32)  # (384,)
            his_emb = self.history_embeddings[idx].astype(np.float32)  # (384,)
            return {"x_enc": x_final, "text_emb": text_emb, "his_emb": his_emb, "raw_lc": raw_lc}, torch.tensor(y, dtype=torch.long)

        # å¼€å¯ä¸€ä¸ª [enhance]
        if self.on_enhance:
            x_final = self._enhance_flux(x_raw)  # (512, 2)
            return {"x_enc": x_final, "text_emb": None, "his_emb": None, "raw_lc": raw_lc}, torch.tensor(y, dtype=torch.long)

        # å¼€å¯ä¸€ä¸ª [mm]
        if self.on_mm_statistics:
            # ä»…æ–‡æœ¬åµŒå…¥ï¼ˆå¹¿æ’­ï¼‰
            text_emb = self.text_embeddings[idx].astype(np.float32)  # (384,)
            return {"x_enc": x_final, "text_emb": text_emb, "his_emb": None, "raw_lc": raw_lc}, torch.tensor(y, dtype=torch.long)

        # å¼€å¯ä¸€ä¸ª [mm_his]
        if self.on_mm_history:
            # ä»…æ–‡æœ¬åµŒå…¥ï¼ˆå¹¿æ’­ï¼‰
            his_emb = self.history_embeddings[idx].astype(np.float32)  # (384,)
            return {"x_enc": x_final, "text_emb": None, "his_emb": his_emb, "raw_lc": raw_lc}, torch.tensor(y, dtype=torch.long)

        return {"x_enc": x_final, "text_emb": None, "his_emb": None, "raw_lc": x_final}, torch.tensor(y, dtype=torch.long)

# å°è£…ä¸ºLightningDataModule
class CustomDataModule(LightningDataModule):
    def __init__(self, root_path, batch_size=16, num_workers=10, encoder=None, on_mm_statistics=False, on_mm_history=False, on_enhance=False, on_test_data_half=False, on_downSample=False):
        super().__init__()
        self.root_path = root_path
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.encoder = encoder
        self.on_mm_statistics = on_mm_statistics
        self.on_mm_history = on_mm_history
        self.on_enhance = on_enhance
        self.on_test_data_half = on_test_data_half
        self.on_downSample = on_downSample

    def setup(self, stage=None):
        if stage == "test":
            self.test_dataset = FluxDataLoader(self.root_path, flag='TEST', encoder=self.encoder,
                                               on_mm_statistics=self.on_mm_statistics, on_enhance=self.on_enhance, on_test_data_half=self.on_test_data_half)
        else:
            self.train_dataset = FluxDataLoader(self.root_path, flag='TRAIN', encoder=self.encoder, on_mm_statistics=self.on_mm_statistics, on_mm_history=self.on_mm_history, on_enhance=self.on_enhance, on_downSample=self.on_downSample)
            self.val_dataset = FluxDataLoader(self.root_path, flag='VAL', encoder=self.encoder, on_mm_statistics=self.on_mm_statistics, on_mm_history=self.on_mm_history, on_enhance=self.on_enhance)
            self.test_dataset = FluxDataLoader(self.root_path, flag='TEST', encoder=self.encoder, on_mm_statistics=self.on_mm_statistics, on_mm_history=self.on_mm_history, on_enhance=self.on_enhance, on_test_data_half=self.on_test_data_half)

    def train_dataloader(self):
        return DataLoader( # len(train_loader) åº”è¯¥æ˜¯ 28479ï¼Œä¸æ˜¯ 523
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            # persistent_workers=True, #é¿å…é‡å¤åˆ›å»ºå­è¿›ç¨‹çš„å¼€é”€
            pin_memory=True,
            collate_fn=collate_fn
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            # persistent_workers=True,  # é¿å…é‡å¤åˆ›å»ºå­è¿›ç¨‹çš„å¼€é”€ï¼ˆæ¯ä¸ª epoch å¼€å§‹æ—¶ï¼‰
            pin_memory=True,
            collate_fn=collate_fn
        )

    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            # persistent_workers=True,  # é¿å…é‡å¤åˆ›å»ºå­è¿›ç¨‹çš„å¼€é”€ï¼ˆæ¯ä¸ª epoch å¼€å§‹æ—¶ï¼‰
            pin_memory=True,
            collate_fn=collate_fn
        )

# å®šä¹‰æ¨¡å‹
class MyTransformerModel(nn.Module):
    def __init__(self, num_classes=2, input_dim=1, model_type="bert", use_lora=False, text_emb_dim=768, use_multimodal=False): #  input_dim
        super().__init__()
        self.model_type = model_type.lower()
        assert self.model_type in ["bert", "gpt2", "roberta"], "model_type is not included." #

        # è·å–æ˜¯å¦å¼€å¯å¤šæ¨¡æ€
        self.use_multimodal = use_multimodal

        # ä»æ˜ å°„è¡¨ä¸­è·å–æœ¬åœ°è·¯å¾„
        LOCAL_MODEL_PATH = MODEL_PATH_MAP.get(model_type)


        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if self.model_type == "bert":
            self.backbone = BertModel.from_pretrained(LOCAL_MODEL_PATH)
        elif self.model_type == "gpt2":  # gpt2
            self.backbone = GPT2Model.from_pretrained(LOCAL_MODEL_PATH)
        elif self.model_type == "roberta":
            self.backbone = RobertaModel.from_pretrained(LOCAL_MODEL_PATH)
        self.config = self.backbone.config

        # å¦‚æœæ²¡æœ‰ç”¨LoRA, åªå¾®è°ƒinput_projå’Œclassifier
        if not use_lora:
            for param in self.backbone.parameters():
                param.requires_grad = False

        # è¾“å…¥æŠ•å½±å±‚å’Œåˆ†ç±»å¤´
        self.input_proj = nn.Linear(input_dim, self.config.hidden_size)
        self.classifier = nn.Sequential(
            nn.Linear(self.config.hidden_size, 256), # åç»­å¯ä»¥è°ƒæ•´ï¼š æ­¤å¤„çš„256æ˜¯ä¸€ä¸ªå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°
            nn.ReLU(),
            nn.Linear(256, num_classes)
        )

        # å¼•å…¥å¤šæ¨¡æ€æ”¹è¿›åï¼Œéœ€è¦å¯¹åº”çš„æ–‡æœ¬èåˆå±‚
        #  å¼•å…¥è½»é‡çº§å¯å­¦ä¹ æ–‡æœ¬ç¼–ç æ¨¡å—
        # === Multimodal Fusion: Text Embedding Compressor ===
        self.text_proj = nn.Linear(text_emb_dim, 512) # out_features : ç‰¹å¾ç»´åº¦
        self.text_act = nn.ReLU()  # optional non-linearity

        # å¦‚æœå½“å‰ä¸ä½¿ç”¨å¤šæ¨¡æ€ï¼Œå†»ç»“è¿™äº›å±‚ï¼
        if not self.use_multimodal:  # å‡è®¾ä½ æœ‰ä¸€ä¸ªæ ‡å¿—ä½ï¼Œæ¯”å¦‚ args.multimodal æˆ– self.hparams.multimodal
            for param in self.text_proj.parameters():
                param.requires_grad = False

        if use_lora:
            if self.model_type == "bert" or self.model_type == "roberta":
                my_target_modules = ["query", "key", "value", "dense"]
            elif self.model_type == "gpt2":
                my_target_modules=["attn.c_attn", "attn.c_proj"]
                # [choice1]
                # target_modules = ["attn.c_attn"],  # ç­‰ä»·äºBERTä¸­çš„queryã€keyå’Œvalue
                # [choice2]
                # target_modules=["c_attn"],#
                # [choice3]
                # target_modules=[
                #     "attn.c_attn",
                #     "attn.c_proj",
                # ],

            peft_config = LoraConfig(
                task_type=None,
                r=8,
                lora_alpha=32,
                lora_dropout=0.1,
                target_modules=my_target_modules,
                bias="none",  # ä¸è®­ç»ƒåŸå§‹æ¨¡å‹çš„ bias å‚æ•°ï¼ˆé»˜è®¤å€¼ï¼Œæœ€å¸¸ç”¨ï¼‰
            )
            self.backbone = get_peft_model(self.backbone, peft_config)
            self.backbone.print_trainable_parameters()

    def forward(self, input_ids, attention_mask=None, text_emb=None, his_emb=None):
        # === . Optional: Inject compressed text as additional channels ===
        x = input_ids
        if text_emb is not None: # æ·»åŠ æ–‡æœ¬ï¼ˆç»Ÿè®¡ä¿¡æ¯ï¼‰åµŒå…¥
            # Compress text: [B, text_dim] -> [B, L]
            text_comp = self.text_act(self.text_proj(text_emb))  # [B, k], k <=4
            x = torch.cat([input_ids, text_comp.unsqueeze(-1)], dim=-1)  # [B, L, C + C]

        if his_emb is not None: # æ·»åŠ æ–‡æœ¬ï¼ˆå†å²åºåˆ—ï¼‰åµŒå…¥
            his_comp = self.text_act(self.text_proj(his_emb))
            x = torch.cat([x, his_comp.unsqueeze(-1)], dim=-1)

        embedded = self.input_proj(x)

        # å¤„ç†attention_mask
        if attention_mask is None:
            attention_mask = torch.ones(
                embedded.shape[:2],  # [B, L]
                dtype=torch.long,
                device=embedded.device
            )

        # å‰å‘ä¼ æ’­
        if self.model_type == "bert" or self.model_type == "roberta":
            outputs = self.backbone(inputs_embeds=embedded) # éœ€è¦æ„é€ çš„ï¼š(batch_size, seq_len, bert_hidden_size)
        elif self.model_type == "gpt2":
            # GPT2 é»˜è®¤æ˜¯ causalï¼Œä½†æˆ‘ä»¬ä¼ å…¥ attention_mask å…¨1ï¼Œç­‰æ•ˆäºåŒå‘ï¼ˆéè‡ªå›å½’ï¼‰
            outputs = self.backbone(
                inputs_embeds=embedded,
                attention_mask=attention_mask  # â† å…³é”®ï¼šç¦ç”¨ causal maskï¼
            )

        # å– [CLS] æˆ–ç¬¬ä¸€ä¸ª token
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # å– [CLS]
        logits = self.classifier(cls_embedding)
        return logits

# å°è£…ä¸ºLightningæ¨¡å‹
class MyTransformerLightningModule(LightningModule):
    def __init__(self, num_classes=2, input_dim=1, model_type="bert", use_lora=False, lr=1e-4, on_phy_loss=False, text_emb_dim=768, use_multimodal=False): # ã€val_dynamic_thresholdã€‘ç»è¿‡å®éªŒï¼Œæ•ˆæœä¸€èˆ¬ï¼Œæš‚æ—¶ä¸è€ƒè™‘
        super().__init__()
        self.save_hyperparameters() # è‡ªåŠ¨ä¿å­˜æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬use_lora
        self.model_type=model_type

        self.model = MyTransformerModel(
            num_classes=num_classes,
            input_dim=input_dim,
            model_type=model_type,
            use_lora=use_lora,
            text_emb_dim=text_emb_dim,
            use_multimodal=use_multimodal
        )
        # éªŒè¯å¯è®­ç»ƒè¡Œ
        print("Input projection requires_grad:", next(self.model.input_proj.parameters()).requires_grad)
        print("Classifier requires_grad:", next(self.model.classifier.parameters()).requires_grad)
        print("Text projection requires_grad:", next(self.model.text_proj.parameters()).requires_grad)

        # å¼•å…¥ç‰©ç†æŸå¤±å‡½æ•°
        self.on_phy_loss = on_phy_loss
        self.criterion = nn.CrossEntropyLoss()
        if self.on_phy_loss:
            self.criterion = PhysicsRegularizedLoss()


        self.validation_outputs = []
        self.test_outputs = []

        # #  éªŒè¯é›†åŠ¨æ€é˜ˆå€¼æœç´¢ï¼šæ–°å¢ -- ç”¨äºä¿å­˜éªŒè¯é›†å®Œæ•´æ¦‚ç‡å’Œæ ‡ç­¾ï¼ˆç”¨äºæ‰¾é˜ˆå€¼ï¼‰
        # self.on_val_dynamic_threshold = on_val_dynamic_threshold  # â† å­˜å‚¨æ ‡å¿— ã€val_dynamic_thresholdã€‘ç»è¿‡å®éªŒï¼Œæ•ˆæœä¸€èˆ¬ï¼Œæš‚æ—¶ä¸è€ƒè™‘
        # self.val_probs = None
        # self.val_trues = None

    def forward(self, input_ids, text_emb=None, his_emb=None):
        return self.model(input_ids, text_emb=text_emb, his_emb=his_emb)

    def _prepare_batch(self, batch):# ç¼–å†™é¢„è§£åŒ…é€»è¾‘
        inputs, label = batch
        x_enc = inputs['x_enc'].float().to(self.device)
        text_emb = inputs['text_emb'].float().to(self.device) if inputs['text_emb'] is not None else None
        his_emb = inputs['his_emb'].float().to(self.device) if inputs['his_emb'] is not None else None
        raw_lc = inputs['raw_lc'].float().to(self.device)
        return x_enc, text_emb, his_emb, label.to(self.device), raw_lc

    def training_step(self, batch, batch_idx):
        x_enc, text_emb, his_emb, y, raw_lc = self._prepare_batch(batch)
        # æµ‹è¯• ï¼š x_enc's shape = (16, 512, 1)

        logits = self(x_enc, text_emb, his_emb)  # âœ… æ­£ç¡®è°ƒç”¨

        # å¼•å…¥ç‰©ç†æŸå¤±åï¼š âœ… æ­£ç¡®è®¡ç®— lossï¼šåœ¨ device ä¸Šè®¡ç®—ï¼Œlabel éœ€ squeeze ä¸”ä¸º long
        if self.on_phy_loss:
            loss = self.criterion(logits, y, raw_lc)
        else:
            loss = self.criterion(logits, y.squeeze().long())

        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x_enc, text_emb, his_emb, y, raw_lc = self._prepare_batch(batch)
        logits = self(x_enc, text_emb, his_emb)

        # å¼•å…¥ç‰©ç†æŸå¤±åï¼š âœ… æ­£ç¡®è®¡ç®— lossï¼šåœ¨ device ä¸Šè®¡ç®—ï¼Œlabel éœ€ squeeze ä¸”ä¸º long
        if self.on_phy_loss:
            loss = self.criterion(logits, y, raw_lc)
        else:
            loss = self.criterion(logits, y.squeeze().long())

        preds = torch.argmax(logits, dim=1)
        probs = F.softmax(logits, dim=1)

        # å­˜å‚¨åˆ° CPUï¼ˆé¿å… OOMï¼‰?
        self.validation_outputs.append({
            'loss': loss,
            'preds': preds.cpu().numpy(),
            'probs': probs.cpu().numpy(),
            'y_true': y.cpu().numpy()
        })
        return loss

    def on_validation_epoch_end(self):
        outputs = self.validation_outputs
        all_y_true = np.concatenate([o['y_true'] for o in outputs])
        all_probs = np.concatenate([o['probs'] for o in outputs])  # shape: (N, 2)

        # # åªæœ‰å¯ç”¨åŠ¨æ€é˜ˆå€¼æ—¶æ‰ä¿å­˜éªŒè¯é›†æ¦‚ç‡å’Œæ ‡ç­¾ ã€val_dynamic_thresholdã€‘ç»è¿‡å®éªŒï¼Œæ•ˆæœä¸€èˆ¬ï¼Œæš‚æ—¶ä¸è€ƒè™‘
        # if self.on_val_dynamic_threshold:
        #     # ä¿å­˜éªŒè¯é›†æ­£ç±»æ¦‚ç‡å’ŒçœŸå®æ ‡ç­¾ï¼ˆç”¨äºåç»­æ‰¾é˜ˆå€¼ï¼‰
        #     self.val_probs = all_probs[:, 1]  # æ­£ç±»æ¦‚ç‡
        #     self.val_trues = all_y_true.flatten()

        # åŸæœ‰æŒ‡æ ‡è®¡ç®—ï¼ˆä»ç”¨é»˜è®¤é˜ˆå€¼=0.5ï¼Œç”¨äºæ—©åœç›‘æ§ï¼‰
        self._compute_and_log_metrics(self.validation_outputs, prefix="val")
        self.validation_outputs.clear()


    def test_step(self, batch, batch_idx):
        x_enc, text_emb, his_emb, y, raw_lc = self._prepare_batch(batch)
        logits = self(x_enc, text_emb, his_emb)

        # å¼•å…¥ç‰©ç†æŸå¤±åï¼š âœ… æ­£ç¡®è®¡ç®— lossï¼šåœ¨ device ä¸Šè®¡ç®—ï¼Œlabel éœ€ squeeze ä¸”ä¸º long
        if self.on_phy_loss:
            loss = self.criterion(logits, y, raw_lc)
        else:
            loss = self.criterion(logits, y.squeeze().long())

        preds = torch.argmax(logits, dim=1)
        probs = F.softmax(logits, dim=1)

        self.test_outputs.append({
            'loss': loss,
            'preds': preds.cpu().numpy(),
            'probs': probs.cpu().numpy(),
            'y_true': y.cpu().numpy()
        })
        return loss

    def _compute_and_log_metrics_with_custom_preds(self, y_true, y_pred, probs, loss, prefix="test", threshold=0.5,
                                                   val_f2=None):
        # åŠ æƒæŒ‡æ ‡
        acc_w = accuracy_score(y_true, y_pred)
        f1_w = f1_score(y_true, y_pred, average='weighted')
        rec_w = recall_score(y_true, y_pred, average='weighted')
        prec_w = precision_score(y_true, y_pred, average='weighted')

        # æ­£ç±»æŒ‡æ ‡
        rec_pos = recall_score(y_true, y_pred, pos_label=1, average='binary')
        prec_pos = precision_score(y_true, y_pred, pos_label=1, average='binary')
        f1_pos = f1_score(y_true, y_pred, pos_label=1, average='binary')
        f2_pos = fbeta_score(y_true, y_pred, beta=2.0, pos_label=1, average='binary')

        # AUCï¼ˆä¸å˜ï¼Œå› ä¸ºç”¨çš„æ˜¯åŸå§‹æ¦‚ç‡ï¼‰
        auc_roc = auc_pr = float('nan')
        if len(np.unique(y_true)) == 2:
            auc_roc = roc_auc_score(y_true, probs[:, 1])
            auc_pr = average_precision_score(y_true, probs[:, 1])

        # æ—¥å¿—
        self.log(f'{prefix}_loss', loss, sync_dist=True)
        self.log(f'{prefix}_accuracy', acc_w, sync_dist=True)
        self.log(f'{prefix}_f1_weighted', f1_w, sync_dist=True)
        self.log(f'{prefix}_recall_pos', rec_pos, sync_dist=True)
        self.log(f'{prefix}_precision_pos', prec_pos, sync_dist=True)
        self.log(f'{prefix}_f1_pos', f1_pos, sync_dist=True)
        self.log(f'{prefix}_f2_pos', f2_pos, sync_dist=True)
        self.log(f'{prefix}_threshold_used', threshold, sync_dist=True)

        if auc_roc != float('nan'):
            self.log(f'{prefix}_auc_roc', auc_roc, sync_dist=True)
            self.log(f'{prefix}_auc_pr', auc_pr, sync_dist=True)

        # æ‰“å°ç»“æœ
        print("\n" + "=" * 60)
        print(f"ã€{prefix.upper()} é›†æœ€ç»ˆç»“æœï¼ˆåŠ¨æ€é˜ˆå€¼ï¼‰ã€‘")
        print("=" * 60)
        print(f"Threshold used: {threshold:.3f}")
        if val_f2 is not None:
            print(f"F2 on val (for threshold selection): {val_f2:.4f}")
        print(f"Loss: {loss:.6f}")
        print("\nã€åŠ æƒæŒ‡æ ‡ï¼ˆæ•´ä½“ï¼‰ã€‘")
        print(f"Accuracy: {acc_w:.6f}")
        print(f"F1 (weighted): {f1_w:.6f}")
        print(f"Recall (weighted): {rec_w:.6f}")
        print(f"Precision (weighted): {prec_w:.6f}")
        print("\nã€æ­£ç±»æŒ‡æ ‡ï¼ˆlabel=1ï¼‰ã€‘ â† æ ¸å¿ƒï¼")
        print(f"Recall (TPR): {rec_pos:.6f}")
        print(f"Precision: {prec_pos:.6f}")
        print(f"F1-score: {f1_pos:.6f}")
        print(f"F2-score: {f2_pos:.6f}")
        print(f"AUC-ROC: {auc_roc:.6f}")
        print(f"AUC-PR: {auc_pr:.6f}")
        print("=" * 60)

        result_text = (
            f"Accuracy: {acc_w:.6f}\n"
            f"Recall (TPR): {rec_pos:.6f}\n"
            f"Precision: {prec_pos:.6f}\n"
            f"F1-score: {f1_pos:.6f}\n"
            f"AUC-ROC: {auc_roc:.6f}\n"
        )

        # å¯é€‰ï¼šä¿å­˜åˆ°æ–‡ä»¶ï¼ˆæ¨¡ä»¿ä½ çš„ç‰ˆæœ¬1ï¼‰
        folder_path = f'./results/testResult_{self.model_type}/'
        os.makedirs(folder_path, exist_ok=True)
        with open(os.path.join(folder_path, 'result_classification.txt'), 'a') as f:
            f.write("-" * 50 + "\n\n")
            f.write(result_text)
            f.write("-" * 50 + "\n\n")

    def on_test_epoch_end(self):
        outputs = self.test_outputs
        all_y_true = np.concatenate([o['y_true'] for o in outputs])
        all_probs = np.concatenate([o['probs'] for o in outputs])
        test_probs_positive = all_probs[:, 1]
        avg_loss = np.mean([o['loss'].item() for o in outputs])

        # é»˜è®¤è¡Œä¸ºï¼šä½¿ç”¨ argmaxï¼ˆå³é˜ˆå€¼=0.5ï¼‰
        # if not self.on_val_dynamic_threshold:
        print("ğŸ“Œ Dynamic threshold disabled. Using default threshold (0.5).")
        test_preds = np.argmax(all_probs, axis=1)
        threshold_used = 0.5
        val_f2_for_th = None
        # else: ã€val_dynamic_thresholdã€‘ç»è¿‡å®éªŒï¼Œæ•ˆæœä¸€èˆ¬ï¼Œæš‚æ—¶ä¸è€ƒè™‘
        #     # å¯ç”¨åŠ¨æ€é˜ˆå€¼ï¼šåœ¨éªŒè¯é›†ä¸Šæœç´¢æœ€ä¼˜ F2 é˜ˆå€¼
        #     if self.val_probs is None or self.val_trues is None:
        #         print("âš ï¸ Warning: Validation data not available. Falling back to threshold=0.5.")
        #         test_preds = (test_probs_positive >= 0.5).astype(int)
        #         threshold_used = 0.5
        #         val_f2_for_th = None
        #     else:
        #         print("ğŸ” Searching optimal threshold on validation set for F2...")
        #         best_f2 = -1
        #         best_th = 0.5
        #         for th in np.arange(0.01, 0.9, 0.01):
        #             pred_val = (self.val_probs >= th).astype(int)
        #             f2 = fbeta_score(
        #                 self.val_trues, pred_val,
        #                 beta=2.0, pos_label=1, average='binary', zero_division=0
        #             )
        #             if f2 > best_f2:
        #                 best_f2 = f2
        #                 best_th = th
        #         threshold_used = best_th
        #         val_f2_for_th = best_f2
        #         test_preds = (test_probs_positive >= best_th).astype(int)
        #         print(f"âœ… Best threshold: {best_th:.3f} (F2={best_f2:.4f} on val)")

        # åŸå…ˆçš„æ³¨é‡Šæ‰ï¼š
        # self._compute_and_log_metrics(self.test_outputs, prefix="test", print_results=True)
        # self.test_outputs.clear()
        # ä½¿ç”¨æœ€ç»ˆé¢„æµ‹ç»“æœè®¡ç®—æŒ‡æ ‡
        self._compute_and_log_metrics_with_custom_preds(
            y_true=all_y_true,
            y_pred=test_preds,
            probs=all_probs,
            loss=avg_loss,
            prefix="test",
            threshold=threshold_used,
            val_f2=val_f2_for_th
        )
        self.test_outputs.clear()

    def _compute_and_log_metrics(self, outputs, prefix="val", print_results=False):
        all_preds = np.concatenate([o['preds'] for o in outputs])
        all_y_true = np.concatenate([o['y_true'] for o in outputs])
        all_probs = np.concatenate([o['probs'] for o in outputs])
        avg_loss = np.mean([o['loss'].item() for o in outputs])

        # åŠ æƒæŒ‡æ ‡
        acc_w = accuracy_score(all_y_true, all_preds)
        f1_w = f1_score(all_y_true, all_preds, average='weighted')
        rec_w = recall_score(all_y_true, all_preds, average='weighted')
        prec_w = precision_score(all_y_true, all_preds, average='weighted')

        # æ­£ç±»æŒ‡æ ‡ï¼ˆlabel=1ï¼‰
        try:
            rec_pos = recall_score(all_y_true, all_preds, pos_label=1, average='binary')
            prec_pos = precision_score(all_y_true, all_preds, pos_label=1, average='binary')
            f1_pos = f1_score(all_y_true, all_preds, pos_label=1, average='binary')
            f2_pos = fbeta_score(all_y_true, all_preds, beta=2.0, pos_label=1, average='binary')
        except ValueError:
            rec_pos = prec_pos = f1_pos = f2_pos = float('nan')

        # AUC
        auc_roc = auc_pr = float('nan')
        if len(np.unique(all_y_true)) == 2:
            auc_roc = roc_auc_score(all_y_true, all_probs[:, 1])
            auc_pr = average_precision_score(all_y_true, all_probs[:, 1])

        # æ—¥å¿—
        self.log(f'{prefix}_loss', avg_loss, sync_dist=True)
        self.log(f'{prefix}_accuracy', acc_w, sync_dist=True)
        self.log(f'{prefix}_f1_weighted', f1_w, sync_dist=True)
        self.log(f'{prefix}_recall_pos', rec_pos, sync_dist=True)
        self.log(f'{prefix}_precision_pos', prec_pos, sync_dist=True)
        self.log(f'{prefix}_f1_pos', f1_pos, sync_dist=True)

        self.log(f'{prefix}_f2_pos', f2_pos, sync_dist=True)

        if auc_roc != float('nan'):
            self.log(f'{prefix}_auc_roc', auc_roc, sync_dist=True)
            self.log(f'{prefix}_auc_pr', auc_pr, sync_dist=True)

        if print_results:
            print("\n" + "=" * 60)
            print(f"ã€{prefix.upper()} é›†æœ€ç»ˆç»“æœã€‘")
            print("=" * 60)
            print(f"Loss: {avg_loss:.6f}")
            print("\nã€åŠ æƒæŒ‡æ ‡ï¼ˆæ•´ä½“ï¼‰ã€‘")
            print(f"Accuracy: {acc_w:.6f}")
            print(f"F1 (weighted): {f1_w:.6f}")
            print(f"Recall (weighted): {rec_w:.6f}")
            print(f"Precision (weighted): {prec_w:.6f}")
            print("\nã€æ­£ç±»æŒ‡æ ‡ï¼ˆlabel=1ï¼‰ã€‘ â† æ ¸å¿ƒï¼")
            print(f"Recall (TPR): {rec_pos:.6f}")
            print(f"Precision: {prec_pos:.6f}")
            print(f"F1-score: {f1_pos:.6f}")
            print(f"F2-score: {f2_pos:.6f}")
            print(f"AUC-ROC: {auc_roc:.6f}")
            print(f"AUC-PR: {auc_pr:.6f}")
            print("=" * 60)

    def configure_optimizers(self):
        return torch.optim.AdamW(self.model.parameters(), lr=self.hparams.lr)

    def save_model(self, path):
        os.makedirs(path, exist_ok=True)
        if self.hparams.use_lora:
            # LoRAæ¨¡å¼ï¼Œä½¿ç”¨PEFTçš„save_pretrainedï¼ˆåªä¿å­˜adapter + configï¼‰
            model_to_save = self.model.backbone.module if hasattr(self.model.backbone,
                                                                  'module') else self.model.backbone
            model_to_save.save_pretrained(path)
            print(f"LoRA adapter saved to {path}")
        else:
            torch.save(self.model.state_dict(), os.path.join(path, 'pytorch_model.bin'))
            print(f"Full model saved to {path}/pytorch_model.bin")


def main(args):
    # è®¾ç½®éšæœºç§å­
    fix_seed = 2025
    random.seed(fix_seed)
    torch.manual_seed(fix_seed)
    np.random.seed(fix_seed)

    # åˆå§‹åŒ–æ•°æ®æ¨¡å—
    data_module = CustomDataModule(
        root_path=args.root_path,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        encoder=args.encoder,
        on_mm_statistics=args.on_mm_statistics,
        on_mm_history=args.on_mm_history,
        on_enhance=args.on_enhance,
        on_test_data_half = args.on_test_data_half,
        on_downSample = args.on_downSample
    )


    # åˆå§‹åŒ–æ¨¡å‹
    model = MyTransformerLightningModule(
        num_classes=args.num_classes,
        input_dim=args.input_dim,
        model_type=args.model_type,
        use_lora=args.use_lora,
        lr=args.lr,
        text_emb_dim=args.text_emb_dim,
        use_multimodal=args.use_multimodal,
        # on_val_dynamic_threshold=args.on_val_dynamic_threshold # ã€val_dynamic_thresholdã€‘ç»è¿‡å®éªŒï¼Œæ•ˆæœä¸€èˆ¬ï¼Œæš‚æ—¶ä¸è€ƒè™‘
    )

    # é…ç½®æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        monitor='val_f1_pos',
        dirpath=args.output_dir,
        filename=f'{args.model_type}-best-model-{{epoch:02d}}-{{val_f2:.4f}}',
        save_top_k=1, # æœ€å¤šä¿ç•™1ä¸ªæœ€å¥½çš„æ¨¡å‹
        mode='max'
    )

    # é…ç½®æ—©åœå›è°ƒï¼ˆè€å¿ƒå€¼10è½®ï¼‰
    early_stopping = EarlyStopping(
        monitor='val_f1_pos',  # ç›‘è§†éªŒè¯å‡†ç¡®ç‡
        patience=10,  # æ—©åœè½®æ•°
        mode='max',  # æœ€å¤§åŒ–å‡†ç¡®ç‡
        verbose=True,
        check_finite=True
    )

    # é…ç½®TensorBoardæ—¥å¿—
    logger = TensorBoardLogger(save_dir='logs', name=f'{args.model_type}')

    # åˆå§‹åŒ–Trainerï¼Œæ·»åŠ æ—©åœå›è°ƒ
    trainer = Trainer(
        max_epochs=args.epochs,
        accelerator='gpu',
        devices="auto", # â† è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰ CUDA_VISIBLE_DEVICES ä¸­çš„ GPU
        callbacks=[checkpoint_callback, early_stopping],
        logger=logger,
        log_every_n_steps=50,
        enable_progress_bar=True,
        # strategy="ddp_find_unused_parameters_true" # <- è‹¥å¹¶éæ‰€æœ‰æ¨¡å‹å‚æ•°éƒ½è¢«ä½¿ç”¨ï¼Œåˆ™å¼€å¯è¿™ä¸ªï¼Œé¿å…å¤šå¡è®­ç»ƒå¤±è´¥
    )

    # åŒºåˆ†è®­ç»ƒå’Œ è¯„ä¼°æ¨¡å¼ã€‚è®­ç»ƒæ¨¡å¼ï¼š
    if not args.model_eval:
        # è®­ç»ƒæ¨¡å‹
        trainer.fit(model, data_module)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆå¦‚æœæœªè¢«æ—©åœï¼‰
        final_model_path = os.path.join(args.output_dir, 'final_model')
        model.save_model(final_model_path)

        # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶æµ‹è¯•
        print("Loading best model for testing...")
        best_model_path = checkpoint_callback.best_model_path
    # è¯„ä¼°æ¨¡å¼ï¼š
    else:
        # best_model_path = "./outputModels/bert_LoRA_MM_ENH_PHY_THR/bert-best-model-epoch=00-val_f2=0.0000.ckpt" # dimå¯¹åº” 384
        best_model_path = "./outputModels/bert_LoRA_MM_ENH_PHY_THR/bert-best-model-epoch=01-val_f2=0.0000.ckpt" # dimå¯¹åº” 768
        data_module.setup(stage='test')

    if best_model_path and os.path.exists(best_model_path):
        # è¿è¡Œæµ‹è¯•
        print("Best model found!!!")

        # ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹
        best_model = MyTransformerLightningModule.load_from_checkpoint(best_model_path)

        # â­ å…³é”®ï¼šé‡æ–°è¿è¡Œ validation loop ä»¥å¡«å…… val_probs / val_trues
        trainer.validate(best_model, data_module.val_dataloader())  # â† æ–°å¢è¿™è¡Œï¼

        trainer.test(best_model, data_module.test_dataloader())

        if not args.model_eval:
            # ä¿å­˜æµ‹è¯•åçš„æœ€ä½³æ¨¡å‹åˆ°ç‹¬ç«‹ç›®å½•ï¼ˆç”¨æ¥éƒ¨ç½²ï¼‰
            deploy_model_dir = os.path.join(args.output_dir, f'best_deploy_model_{args.model_type}_textEncoder_{args.encoder}')
            best_model.save_model(deploy_model_dir)
            print(f"Deployment model saved to: {deploy_model_dir}")
    else:
        print("No best model found, using last trained model for testing")

        # â­ å…³é”®ï¼šé‡æ–°è¿è¡Œ validation loop ä»¥å¡«å…… val_probs / val_trues
        trainer.validate(model, data_module.val_dataloader())  # â† æ–°å¢è¿™è¡Œï¼

        # ä½¿ç”¨æœ€åè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
        trainer.test(model, data_module.test_dataloader())

        if not args.model_eval:
            deploy_model_dir = os.path.join(args.output_dir, f'last_deploy_model_{args.model_type}')
            model.save_model(deploy_model_dir)
            print(f"Last model saved for deployment: {deploy_model_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Stellar Forecasting with LLM and LoRA using PyTorch Lightning')

    # æ•°æ®å‚æ•°
    parser.add_argument('--root_path', type=str, default='./myDataK', help='Path to my data')
    parser.add_argument('--output_dir', type=str, default='./final_output_models_kep', help='Output directory for saved model')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--num_classes', type=int, default=2, help='Number of output classes')
    parser.add_argument('--input_dim', type=int, default=1, help='Input feature dimension') # æ¨¡å‹è¾“å…¥ç»´åº¦ï¼Œéšç€æ”¹è¿›ç‚¹çš„æ·»åŠ è€Œçµæ´»æ”¹å˜

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=64, help='Batch size per GPU')
    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--epochs', type=int, default=100, help='Number of training epochs (æ—©åœå¯èƒ½æå‰ç»ˆæ­¢)') # è®­ç»ƒè½®æ¬¡è®¾ç½®ä¸º100ï¼Œä½†ç»å¸¸epoch=10æ—¶æ—©åœ
    parser.add_argument('--num_workers', type=int, default=4, help='Number of data loading workers') # æš‚æ—¶ä¿®æ”¹ä¸º0 å¦åˆ™æ”¹ä¸º4

    # ä»¥ä¸‹æ˜¯è‡ªå®šä¹‰å‚æ•°ï¼Œæ–¹ä¾¿å¯¹ç¨‹åºè¿›è¡Œæ”¹è¿›å’Œè°ƒè¯•
    # æ·»åŠ å®éªŒæ¬¡æ•°ï¼Œç”¨äºä¿å­˜ç›¸å…³æ¨¡å‹ exp_num
    parser.add_argument('--exp_num', type=int, default=1, help='exm num, we need to run 3 times')

    # æ˜¯å¦ä½¿ç”¨loraå¾®è°ƒ
    parser.add_argument("--use_lora", action="store_true", help="Enable LoRA fine-tuning")

    # æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
    parser.add_argument("--debug", action="store_true", help="Epoch is 1 for debugging")

    # å®šä¹‰è°ƒç”¨çš„æ¨¡å‹: bertã€gpt2ã€roberta
    parser.add_argument('--model_type', type=str, default="bert", help='Model type')

    # æ˜¯å¦å¼€å¯å¤šæ¨¡æ€æ¨¡å¼ï¼šuse_multimodal, å…¶å¯¹åº”ä¸¤ç±»ï¼šï¼ˆç»Ÿè®¡ä¿¡æ¯+å†å²åºåˆ—ï¼‰ï¼Œåˆ†åˆ«å¯¹åº”--on_mm_statisticsã€--on_mm_history
    parser.add_argument('--use_multimodal', action='store_true', help='Enable multimodal input (x_enc + text_emb[stastic])')
    parser.add_argument('--on_mm_statistics', action='store_true', help='Enable multimodal input (x_enc + text_emb[stastic])')
    parser.add_argument('--on_mm_history', action='store_true', help='Enable multimodal input (x_enc + text_emb[history])')

    # è‹¥å¼€å¯å¤šæ¨¡æ€ï¼Œåˆ™éœ€è¦äº‹å…ˆè®¡ç®—æ–‡æœ¬ç¼–ç å‘é‡å¹¶å­˜å…¥ç›¸å…³æ–‡ä»¶å¤¹ã€‚ï¼ˆæ‰§è¡Œæœ¬ç›®å½•ä¸‹çš„generate_text_embeddings.py and generate_history_embeddings.pyæ–‡ä»¶ï¼‰
    parser.add_argument('--encoder', type=str, default="bert", help='type of encoder we use.')
    parser.add_argument('--text_emb_dim', type=int, default=768, help='type of encoder we use.')  # æŒ‡å®šå…¶ç‰¹å¾ç»´åº¦

    # æ˜¯å¦å¼€å¯å•æ¨¡æ€ç‰¹å¾å¢å¼º
    parser.add_argument('--on_enhance', action='store_true', help='Enable flux augmentation(Add å·®åˆ†)')

    # æ˜¯å¦å¼€å¯ç‰©ç†æŸå¤±å‡½æ•°çº¦æŸ
    parser.add_argument('--on_phy_loss', action='store_true', help='Enable physical loss')

    # # æ˜¯å¦å¼€å¯éªŒè¯é›†åŠ¨æ€é˜ˆå€¼ï¼ˆç”¨äºåˆ†ç±»ä»»åŠ¡ä¸­è‡ªåŠ¨è°ƒæ•´å†³ç­–é˜ˆå€¼ï¼‰ == ã€val_dynamic_thresholdã€‘ç»è¿‡å®éªŒï¼Œæ•ˆæœä¸€èˆ¬ï¼Œæš‚æ—¶ä¸è€ƒè™‘
    # parser.add_argument('--on_val_dynamic_threshold', action='store_true',
    #                     help='Enable dynamic threshold tuning on validation set')

    # æ·»åŠ æµ‹è¯•é›†æ•°æ®ä¸€åŠè°ƒæ•´ï¼ˆæµ‹è¯•é›†ä¿è¯æ­£æ ·æœ¬å æœ‰ç‡ä¸º50%ï¼‰ æ•°æ®é›†å·²ç»æ›´æ–°ã€æ¯”ä¾‹ä¸º50%ã€‘
    parser.add_argument('--on_test_data_half', action='store_true',
                        help='Enable dynamic threshold tuning on validation set')
    # æ·»åŠ æ˜¯å¦æ¬ é‡‡æ ·è®­ç»ƒæ•°æ®
    parser.add_argument('--on_downSample', action='store_true',
                        help='Downsample train_data')

    # å¦‚æœæœ‰ç°æœ‰çš„æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥æµ‹è¯•ï¼Œåˆ™æ‰“å¼€è¿™ä¸ªå‚æ•°é¡¹
    parser.add_argument('--model_eval', action='store_true',
                        help='Enable dynamic threshold tuning on validation set')

    # åŠ¨æ€é€‰æ‹©æˆ‘ä»¬æ‰€éœ€è¦çš„æ•°æ®é›†
    parser.add_argument('--dataset', type=str, default="kepler", help='dataset we use.')

    args = parser.parse_args()

    if args.dataset == "kepler":
        args.root_path = "./myDataK"
    elif args.dataset == "tess":
        args.root_path = "./myDataT"
    else:
        raise ValueError(f"Unsupported dataset: {args.dataset}")

    if args.on_mm_statistics or args.on_mm_history:
        args.use_multimodal = True

    # æ ¹æ®åˆ›æ–°ç‚¹é€‰æ‹©è¾“å…¥ç»´åº¦
    if args.on_enhance and args.on_mm_statistics and args.on_mm_history:
        args.input_dim = 4
    elif args.on_enhance and args.on_mm_statistics:
        args.input_dim = 3
    elif args.on_enhance and args.on_mm_history:
        args.input_dim = 3
    elif  args.on_mm_statistics and args.on_mm_history:
        args.input_dim = 3
    elif args.on_mm_statistics or args.on_mm_history or args.on_enhance :
        args.input_dim = 2

    # æ ¹æ®æ‰€é€‰çš„æ¨¡å‹ï¼Œè‡ªåŠ¨è®¾ç½®å…¶è¾“å…¥ç»´åº¦
    ENCODER_DIM_MAP = {
        "minLM": 384,
        "bert-chinese": 768,
        # â¡ï¸ æœªæ¥å¯åŠ ï¼š "bge": "./textEncoder/bge-small-en-v1.5", ...
    }
    args.text_emb_dim = ENCODER_DIM_MAP[args.encoder]

    # å…³é”®é…ç½®é«˜äº®å±•ç¤º
    print("\n" + "=" * 60)
    print("ğŸ”‘ Key Experimental Settings:")
    print(f"  â¤ Multimodal (text[statistics] + LC):              {'âœ… ON' if args.on_mm_statistics else 'âŒ OFF'}")
    print(f"  â¤ Multimodal-history (text[history] + LC):              {'âœ… ON' if args.on_mm_history else 'âŒ OFF'}")
    print(f"  â¤ Time Series Enhancement (Î”flux):     {'âœ… ON' if args.on_enhance else 'âŒ OFF'}")
    print(f"  â¤ Physics-Regularized Loss:            {'âœ… ON' if args.on_phy_loss else 'âŒ OFF'}")
    # print(f"  â¤ Dynamic Validation Threshold:        {'âœ… ON' if args.on_val_dynamic_threshold else 'âŒ OFF'}") # ã€val_dynamic_thresholdã€‘ç»è¿‡å®éªŒï¼Œæ•ˆæœä¸€èˆ¬ï¼Œæš‚æ—¶ä¸è€ƒè™‘
    print("=" * 60 + "\n")

    # æ„å»ºæ”¹è¿›ç‚¹çš„æ ‡ç­¾ï¼šç”¨äºä¿å­˜è®­ç»ƒåçš„ç»“æœ
    features_tags = []
    if args.on_mm_statistics:
        features_tags.append("1MMs")  # MultiModal
    if args.on_phy_loss:
        features_tags.append("2PHY")  # Physics loss
    if args.on_mm_history:
        features_tags.append("3MMh")  # MultiModal
    if args.on_enhance:
        features_tags.append("4ENH")  # Enhancement
    # if args.on_val_dynamic_threshold: # ã€val_dynamic_thresholdã€‘ç»è¿‡å®éªŒï¼Œæ•ˆæœä¸€èˆ¬ï¼Œæš‚æ—¶ä¸è€ƒè™‘
    #     features_tags.append("THR")  # Dynamic Threshold

    feature_str = "_".join(features_tags) if features_tags else "BASE"

    # å°è£…ä¸€ä¸‹æ ‡ç­¾ï¼ŒåŠ å…¥æ¨¡å‹ç±»å‹ã€LoRA
    output_model_id = (
        f"{args.model_type}_"
        f"{'LoRA' if args.use_lora else 'NoLoRA'}_"
        f"{'train_downSample' if args.on_downSample else 'Normal50Percent'}_"
        f"{feature_str}_"
        f"{args.exp_num}"
    )

    # âœ… å¦‚æœå¯ç”¨äº† debug æ¨¡å¼ï¼Œè®¾ç½® epochs ä¸º 1ï¼Œå¹¶ä¸”æ¨¡å‹ä¿å­˜è·¯å¾„ååŠ  _DEBUG åç¼€é¿å…æ±¡æŸ“
    if args.debug:
        output_model_id += "_DEBUG"
        args.epochs = 1
        args.num_workers = 0
        print(f"[DEBUG MODE] Setting epochs to {args.epochs}")


    # æ„å»ºæœ€ç»ˆè¾“å‡ºè·¯å¾„ å¹¶ç¡®ä¿ç›®å½•å­˜åœ¨
    args.output_dir = os.path.join(args.output_dir, output_model_id)
    os.makedirs(args.output_dir, exist_ok=True)

    # ä¸»å‡½æ•°
    main(args)
